hello everyone in this video you'll

learn all about large language models

let's start with that autocomplete

feature on your mobile phone

did you ever wonder how it works

the suggested word here is the

it's the most used word in the english

language

let's type y next

there are a number of words that start

with t y

if you took the shortest you get t y e

this graph plots the frequency of this

word over time it was pretty popular in

1806 but today you'd have to read 20

million words to find your first

occurrence of tye

we can look up these frequencies for

every word starting with t y

and if we sort by word frequency we have

a clear winner

the same approach works for phrases and

sentences

you see it every time you start typing

into your internet search engine

the search engine scores each query by

calculating its frequency in other words

how many other people have used that

query

so

is that all there is to language

modeling

indeed the goal is to assign a

probability to every sentence

and frequencies are one way to multiple

probabilities

the problem is that this frequency

approach doesn't allow you to score new

sentences

this is a perfectly valid sentence but

one that may not have appeared

previously

you may ask how many new sentences are

there really

given the zillions of internet posts

each day won't we exhaust all possible

combinations of words soon

let's do a back of the envelope

calculation

there are well over a hundred thousand

words in the english language and a

typical sentence has more than 10 words

that means 10 to the 50th combinations

this is a ridiculously big number

the vast majority of sentences will

never be seen by any human

so to really model language we need to

do more than just count sentences that

already exist

we need to somehow model things like

grammar and style

here's an example from a noble

prize-winning poet

let's try to build a language model that

can write like bob dylan

we'll start by thinking of this text as

a time series

where each word depends on the previous

one

notice that the word was appears three

times

let's merge all three instances into one

this turns our series into a graph

the other word that repeats is if

let's merge these two as well

if we add probability to these edges it

becomes a language model

and you can use it to generate text in

the style of bob dylan

if you start it early

and traverse the right sequence of

branches

you'll get the original verses

now let's generate a brand new phrase

that wasn't in the song

we'll start at the and follow these

branches

here's another example

hey these are actually pretty good

they are brand new phrases that sound

like bob dylan

but other paths give bizarre results

and most just produce nonsense

how can we make this better

for starters we can use a lot more text

to build our model

here's what you get if you build the

model using the whole song

hmm

these are still pretty weird

the real problem here is that our model

is too simplistic

it assumes that each word depends only

on the previous word

we can write this as a conditional

probability

it's the probability of the current word

x of n conditioned on the previous word

x of n minus one

you can do a little better if you look

at relationships of three words instead

of just two

let's build a table of all consecutive

triples

these are called trigrams

and you can use these trigrams to define

next word probabilities based on the two

previous words

the text generation results are slightly

better but still not great

the problem is that these words can have

pretty long range dependencies

for example the word red relates to hair

which is three words back but it also

rhymes with bed which is 13 words back

if you ignored those rhymes the song

wouldn't work at all

so we need to model functions like this

or even longer to accurately represent

language

and these functions are exceedingly

complex

there's no way we can model this exactly

but we can try to approximate it

there are a number of ways to

approximate a function

you may have heard of fourier series

which approximates a function using

sines and cosines

or taylor series which is a sum of

polynomials

these are both universal approximators

they can approximate almost any function

another example of a universal

approximator is neural networks

an advantage of neural networks is that

you don't need to know anything about

the function you're trying to

approximate just input and output pairs

let's try an example

this function is kind of interesting

let's approximate it with a neural

network

we'll use this one it has five nodes and

eight weights

let's choose an x position on the graph

and send that through the network

the first layer duplicates x and

multiplies each copy with a different

weight

then each weighted copy is passed

through an activation function an

s-shaped curve called a sigmoid in this

case

multiply by a new weight

and then add it up

the result is our approximation of f of

x which we'll call y

and we can plot it to see how far it is

from the function we're trying to fit

this is our error

you can see it's pretty big

that's because i generated the weights

randomly

so this is just one data point

we can send many x values to our network

to generate this red curve

let's define an error function that adds

up all of these differences between red

and blue curve values

we'll use these errors to update the

weights

this is called training the network and

if we repeat these update steps

thousands of times

we get a pretty good fit

we can think of our energy function as a

landscape

our goal is to find the lowest point the

basin

that seems easy right

but what if i didn't show you the

function

it's a lot harder now huh

okay i'll give you a few points

and also they're negative gradients

which point downhill

now you can just follow the gradients to

roll down the hill

this is called gradient descent

it's how neural networks are optimized

so to train our network we need the

gradient of the error function

it's a vector with eight partial

derivatives one for each weight

this turns out to be pretty

straightforward to compute for neural

networks and you can do it in a single

backwards path through the network

this process of calculating partial

derivatives in a network is called back

propagation and it's the workhorse of

neural networks

all right now you just told me that

neural networks are universal

approximators you can fit any function

so how about this function which models

language

well there's some network that can do it

but it needs enough capacity

as an analogy suppose we tried fitting

our blue curve with this network which

only has four weights

it can't fit that second bump because

there's not enough capacity in the

network

and the design decisions matter

like what if i used a different

activation function

relus are pretty popular

but they give piecewise linear

reconstructions

so you need more of them to fit a curvy

function like this

so how do we go about designing a neural

network to model language

stay tuned for part two

where we create an amazing neural

network that can generate poetry

translate languages and even write

computer code

